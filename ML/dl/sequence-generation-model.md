











# 序列到序列模型

在序列生成任务中，有一类任务是序列到序列生成任务，即输入一个序列，生成另一个序列，比如机器翻译、语音识别、文本摘要、对话系统、图像标题生成等。

**序列到序列(Sequence-to-Sequence , Seq2Seq)**是一种条件的序列生成问题，给定一个序列$$\pmb x_{1∶S}$$，生成另一个序列$$\pmb y_{1∶T}$$。输入序列的长度$$S$$和输出序列的长度$$T$$可以不同，比如在机器翻译中，输入为源语言，输出为目标语言。下图给出了基于循环神经网络的序列到序列机器翻译示例，其中$$⟨EOS⟩$$表示输入序列的结束，虚线表示用上一步的输出作为下一步的输入。

![](https://i.loli.net/2020/11/10/jHZJGxo6Wmc7aSn.png)

序列到序列模型的目标是估计条件概率
$$
p_\theta(\pmb y_{1∶T}|\pmb x_{1∶S})=\prod_{t=1}^T p_{\theta}(y_t|\pmb y_{1∶(t-1)},\pmb x_{1∶S})
$$
其中$$\pmb y_t ∈\mathcal{V}$$为词表$$\mathcal{V}$$中的某个词。

给定一组训练数据$$\{(\pmb x_{S_n}, \pmb y_{T_n})\}^N_{n=1}$$，我们可以使用最大似然估计来训练模型参数
$$
\hat{\theta}=\arg\max_\theta\sum_{n=1}^N\log p_\theta(\pmb y_{1:T_n}|\pmb x_{1:S_n})
$$
一旦训练完成，模型就可以根据一个输入序列$$x$$来生成最可能的目标序列
$$
\hat{\pmb y} =\arg\max_y p_{\hat{\theta}}(\pmb y|\pmb x)
$$
具体的生成过程可以通过贪婪方法或束搜索来完成。

和一般的序列生成模型类似，条件概率$$p_θ (y_t|\pmb y_{1∶(t−1)},\pmb x_{1∶S})$$可以使用各种不同的神经网络来实现。这里我们介绍三种主要的序列到序列模型：基于循环神经网络的序列到序列模型、基于注意力的序列到序列模型、基于自注意力的序列到序列模型。



## 基于循环神经网络的序列到序列模型

实现序列到序列的最直接方法是使用两个循环神经网络来分别进行编码和解码，也称为**编码器 - 解码器(encoder-decoder)**模型。

**编码器**

首先使用一个循环神经网络$$f_{\rm enc}$$来编码输入序列$$\pmb x_{1∶S}$$得到一个固定维数的向量$$\pmb u$$，$$\pmb u$$一般为<u>编码循环神经网络最后时刻的隐状态</u>。
$$
\pmb h_{{\rm enc},s}=f_{\rm enc}(\pmb h_{{\rm enc},s-1},\pmb x_{s-1},\theta_{\rm enc}),\ s=1,\cdots,S\\
\pmb u=\pmb h_{{\rm enc},S}
$$
其中$$f_{\rm enc}(⋅)$$为<u>编码循环神经网络</u>，可以为 LSTM 或 GRU ，其参数为$$\theta_{\rm enc}$$，$$\pmb x_{s-1}$$为词$$x$$的词向量。

**解码器**

在生成目标序列时，使用另外一个循环神经网络$$f_{\rm dec}$$来进行解码。在解码过程的第$$t$$步时，已生成前缀序列为$$\pmb y_{1:T_n}$$ . 令$$\pmb h_{{\rm dec},t}$$表示在网络$$f_{\rm dec}$$的隐状态，$$\pmb o_t ∈ (0, 1)^{|\mathcal{V}|}$$为词表中所有词的后验概率，则
$$
\pmb h_{{\rm dec},0}=\pmb u=\pmb h_{{\rm enc},S}\\
\pmb h_{{\rm dec},t}=f_{\rm dec}(\pmb h_{{\rm dec},t-1},\pmb y_{t-1},\theta_{\rm dec})\\
\pmb o_t=g(\pmb h_{{\rm dec},t},\theta_o),\ t=1,\cdots,T
$$
其中$$f_{\rm dec}(\cdot)$$为<u>解码循环神经网络</u>，$$g(⋅)$$为最后一层为 Softmax 函数的前馈神经网络，$$\theta_{\rm dec}$$和$$θ_o$$为网络参数，$$\pmb y_{t-1}$$为词$$y$$的词向量，$$\pmb y_0$$为一个特殊符号，比如$$⟨EOS⟩$$。

基于循环神经网络的序列到序列模型的缺点是：(1) 编码向量$$\pmb u$$的容量问题，输入序列的信息很难全部保存在一个固定维度的向量中； (2) 当序列很长时， 由于循环神经网络的长程依赖问题，容易丢失输入序列的信息。



## 基于注意力的序列到序列模型

为了获取更丰富的输入序列信息，我们可以在每一步中通过注意力机制来从输入序列中选取有用的信息。

在解码过程的第$$t$$步中，先用上一步的隐状态$$\pmb h_{{\rm dec},t-1}$$作为查询向量，利用注意力机制从所有输入序列的隐状态$$H_{\rm enc}=[\pmb h_{{\rm enc},1},\cdots,\pmb h_{{\rm enc},S}]$$中选择相关信息
$$
\pmb c_t={\rm att}(H_{\rm enc},\pmb h_{{\rm dec},t-1})\\
=\sum_{i=1}^S \alpha_{i}\pmb h_{{\rm enc},i} \\
=\sum_{i=1}^S {\rm softmax}(s(\pmb h_{{\rm enc},i},\pmb h_{{\rm dec},t-1}))\pmb h_{{\rm enc},i}\\
$$
其中$$s(\cdot)$$为注意力打分函数。

然后将从输入序列中选择的信息$$\pmb c_t$$也作为解码器$$f_{\rm dec}(\cdot)$$在第$$t$$步时的输入，得到第$$t$$步的隐状态
$$
\pmb h_{{\rm dec},t}=f_{\rm dec}(\pmb h_{{\rm dec},t-1},[\pmb y_{t-1}, \pmb c_t],\theta_{\rm dec})\\
$$
最后将$$\pmb h_{{\rm dec},t}$$输入到分类器$$g(\cdot)$$中来预测词表中每个词出现的概率。



## 基于自注意力的序列到序列模型

除长程依赖问题外，基于循环神经网络的序列到序列模型的另一个缺点是无法并行计算。为了提高并行计算效率以及捕捉长距离的依赖关系，我们可以使用**自注意力模型(self-attention model)**来建立一个全连接的网络结构. 本节介绍一个目前非常成功的基于自注意力的序列到序列模型：Transformer [Vaswani et al., 2017]。

……