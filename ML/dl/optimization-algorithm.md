虽然神经网络具有非常强的表达能力，但是当应用神经网络模型到机器学习时依然存在一些难点问题。主要分为两大类：
(1) 优化问题：深度神经网络的优化十分困难。首先，神经网络的损失函数是一个非凸函数，找到全局最优解通常比较困难；其次，深度神经网络的参数通常非常多，训练数据也比较大，因此也无法使用计算代价很高的二阶优化方法，而一阶优化方法的训练效率通常比较低。此外，深度神经网络存在梯度消失或爆炸问题，导致基于梯度的优化方法经常失效。

(2) 泛化问题：由于深度神经网络的复杂度比较高，并且拟合能力很强，很容易在训练集上产生过拟合。因此在训练深度神经网络时，同时也需要通过一定的正则化方法来改进网络的泛化能力。目前，研究者从大量的实践中总结了一些经验方法，在神经网络的表示能力、复杂度、学习效率和泛化能力之间找到比较好的平衡，并得到一个好的网络模型。





# 网络优化

网络优化是指寻找一个神经网络模型来使得经验（或结构）风险最小化的过程，包括模型选择以及参数学习等。深度神经网络是一个高度非线性的模型，其风险函数是一个非凸函数，因此风险最小化是一个非凸优化问题。此外， 深度神经网络还存在梯度消失问题。因此，深度神经网络的优化是一个具有挑战性的问题。



## 网络结构多样性

神经网络的种类非常多，比如卷积网络、循环网络、图网络等。不同网络的结构也非常不同，有些比较深，有些比较宽。不同参数在网络中的作用也有很大的差异，比如连接权重和偏置的不同，以及循环网络中循环连接上的权重和其他权重的不同。
由于网络结构的多样性，我们很难找到一种通用的优化方法。不同优化方法在不同网络结构上的表现也有比较大的差异。
此外，网络的超参数一般比较多，这也给优化带来很大的挑战。



## 高维变量的非凸优化

低维空间的非凸优化问题主要是存在一些局部最优点。基于梯度下降的优化方法会陷入局部最优点，因此在低维空间中非凸优化的主要难点是如何选择初始化参数和逃离局部最优点。深度神经网络的参数非常多，其参数学习是在
非常高维空间中的非凸优化问题，其挑战和在低维空间中的非凸优化问题有所不同。

**鞍点**

在高维空间中，非凸优化的难点并不在于如何逃离局部最优点，而是如何逃离**鞍点(saddle point)** [Dauphin et al., 2014]。鞍点的梯度是 0 ，但是在一些维度上是极大值，在另一些维度上是极小值，如下图所示。

图7.1

在高维空间中，局部最小值(local minima)要求在每一维度上都是最低点，这种概率非常低。假设网络有 10000 维参数，梯度为 0 的点（即驻点(stationary point)）在某一维上是局部最小值的概率为 $$p$$（在一般的非凸问题中，$$p\approx 0.5$$），那么在整个参数空间中，驻点是局部最优点的概率是$$p^{10000}$$，这种可能性非常小。也就是说，在高维空间中大部分驻点都是鞍点。
基于梯度下降的优化方法会在鞍点附近接近于停滞，很难从这些鞍点中逃离。因此，<u>随机梯度下降</u>对于高维空间中的非凸优化问题十分重要，通过<u>在梯度方向上引入随机性，可以有效地逃离鞍点</u>。

**平坦最小值**

深度神经网络的参数非常多，并且有一定的冗余性，这使得每单个参数对最终损失的影响都比较小，因此会导致损失函数在局部最小解附近通常是一个平坦的区域，称为**平坦最小值(flat minima)** [Hochreiter et al., 1997; Li
et al., 2017a]。下图给出了平坦最小值和尖锐最小值(sharp minima)的示例。

图7.2

在一个平坦最小值的邻域内，所有点对应的训练损失都比较接近，表明我们在训练神经网络时不需要精确地找到一个局部最小解只要在一个局部最小解的邻域内就足够了。平坦最小值通常被认为和模型泛化能力有一定的关系，一般而言，当一个模型收敛到一个平坦的局部最小值时，其鲁棒性会更好，即微小的参数变动不会剧烈影响模型能力；而当一个模型收敛到一个尖锐的局部最小值时，其鲁棒性也会比较差。具备良好泛化能力的模型通常应该是鲁棒的，因此理想的局部最小值应该是平坦的。

> 这里的很多描述都是经验性的，并没有很好的理论证明。

**局部最小解的等价性**
在非常大的神经网络中，大部分的局部最小解是等价的，它们在测试集上性能都比较相似。此外，局部最小解对应的训练损失都可能非常接近于全局最小解对应的训练损失 [Choromanska et al., 2015] 。虽然神经网络有一定概率收敛于比较差的局部最小值，但随着网络规模增加，网络陷入比较差的局部最小值的概率会大大降低。在训练神经网络时，我们通常没有必要找全局最小值，这反而可能导致过拟合。


