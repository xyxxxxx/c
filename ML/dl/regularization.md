**正则化(regularization)**是一类通过限制模型复杂度，从而避免过拟合，提高泛化能力的方法，比如引入约束、增加先验、提前停止等。

在传统的机器学习中，提高泛化能力的方法主要是限制模型复杂度，比如采用$$\ell_1$$和$$\ell_2$$正则化等方式。而在训练深度神经网络时，特别是在**过度参数化(over-parameterization)**时，$$\ell_1$$和$$\ell_2$$正则化的效果往往不如浅层机器学习模型中显著。因此训练深度学习模型时，往往还会使用其他的正则化方法，比如数据增强、 提前停止、 丢弃法、 集成法等。

> 过度参数化是指模型参数的数量远远大于训练数据的数量。



## $$\ell_1$$和$$\ell_2$$正则化

$$\ell_1$$和$$\ell_2$$正则化是机器学习中最常用的正则化方法，通过约束参数的$$\ell_1$$和$$\ell_2$$范数来减小模型在训练数据集上的过拟合现象。

通过加入$$\ell_1$$和$$\ell_2$$正则化，优化问题可以写为
$$
\pmb\theta^*=\arg \min_{\pmb \theta} \frac{1}{N}\sum_{i=1}^N\mathcal{L}(y^{(i)},f(\pmb x^{(i)};\pmb \theta))+\lambda\ell_p(\pmb \theta)
$$
其中$$\mathcal{L}()$$为损失函数，$$N$$为训练样本数量，$$f()$$为待学习的神经网络，$$\pmb \theta$$为其参数，$$\ell_p$$为范数函数，$$p$$通常取1或2代表$$\ell_1$$和$$\ell_2$$范数，$$\lambda$$为正则化系数。

带正则化的优化问题等价于下面带约束条件的优化问题，
$$
\pmb\theta^*=\arg \min_{\pmb \theta} \frac{1}{N}\sum_{i=1}^N\mathcal{L}(y^{(i)},f(\pmb x^{(i)};\pmb \theta))\\
{\rm s.t.}\quad \ell_p(\pmb\theta)\le 1
$$
$$\ell_1$$范数在零点不可导，因此经常用下式来近似：
$$
\ell_1(\pmb\theta)=\sum_{i=1}^D\sqrt{\theta_i^2+\varepsilon}
$$
其中$$\varepsilon$$为非常小的正常数。

下图给出了不同范数约束条件下的最优化问题示例，可以看到，$$\ell_1$$正则化项通常会使得最优解位于坐标轴上，从而使得最终的参数为稀疏性向量。

![](https://img2018.cnblogs.com/blog/71977/202001/71977-20200101161322601-524903143.png)

![](https://img2018.cnblogs.com/blog/71977/202001/71977-20200101161323095-2101121814.png)

一种折中的正则化方法是同时加入$$\ell_1$$和$$\ell_2$$正则化项，称为**弹性网络正则化(elastic net regularization)** [Zou et al., 2005]。
$$
\pmb\theta^*=\arg \min_{\pmb \theta} \frac{1}{N}\sum_{i=1}^N\mathcal{L}(y^{(i)},f(\pmb x^{(i)};\pmb \theta))+\lambda_1\ell_1(\pmb \theta)+\lambda_2\ell_2(\pmb \theta)
$$
其中$$λ_1$$和$$λ_2$$分别为两个正则化项的系数。



## 权重衰减

**权重衰减(weight decay)**是一种有效的正则化方法[Hanson et al., 1989]，在每次参数更新时，引入一个衰减系数
$$
\pmb\theta_t \leftarrow (1-\beta)\pmb\theta_{t-1}-\alpha\pmb g_t
$$
其中$$\pmb g_t$$是第$$t$$步更新时的梯度，$$\alpha$$为学习率，$$\beta$$为权重衰减系数，一般取值比较小，比如 0.0005。在标准的随机梯度下降中，权重衰减正则化和$$\ell_2$$正则化的效果相同。因此，权重衰减在一些深度学习框架中通过$$\ell_2$$正则化来实现。但是，在较为复杂的优化方法(比如Adam)中，权重衰减正则化和$$\ell_2$$正则化并不等价[Loshchilov et al., 2017b]。



## 提前停止

**提前停止(early stop)**对于深度神经网络来说是一种简单有效的正则化方法。由于深度神经网络的拟合能力非常强，因此比较容易在训练集上过拟合。在使用梯度下降法进行优化时，我们可以使用一个和训练集独立的样本集合，称为**验证集(validation set)**，并用验证集上的错误来代替期望错误。当验证集上的错误率不再下降，就停止迭代。

然而在实际操作中，验证集上的错误率变化曲线并不一定是如图所示的平衡曲线，很可能是先升高再降低。因此，提前停止的具体停止标准需要根据实际任务进行优化 [Prechelt, 1998]。

![](https://i.loli.net/2020/09/04/QTfOXJ9toj6lNFR.png)



## 丢弃法

当训练一个深度神经网络时，我们可以随机丢弃一部分神经元（同时丢弃其对应的连接边）来避免过拟合，这种方法称为丢弃法(dropout method) [Srivastava et al., 2014]。每次选择丢弃的神经元是随机的。最简单的方法是设置一个固定的概率$$p$$，对每一个神经元都以概率$$p$$来判定要不要保留。对于一个神经层$$\pmb y = f(Wx + b)$$，我们可以引入一个掩蔽函数 mask(⋅) 使得$$y =f(W mask(x) + b)$$。掩蔽函数 mask(⋅) 的定义为
$$

$$
其中 m ∈ {0, 1} D 是丢弃掩码( Dropout Mask ), 通过以概率为 p 的伯努利分
布随机生成. 在训练时, 激活神经元的平均数量为原来的 p 倍. 而在测试时, 所有
的神经元都是可以激活的, 这会造成训练和测试时网络的输出不一致. 为了缓解
这个问题, 在测试时需要将神经层的输入 x 乘以 p , 也相当于把不同的神经网络
做了平均. 保留率 p 可以通过验证集来选取一个最优的值. 一般来讲, 对于隐藏
层的神经元, 其保留率 p = 0.5 时效果最好, 这对大部分的网络和任务都比较有
效. 当 p = 0.5 时, 在训练时有一半的神经元被丢弃, 只剩余一半的神经元是可以
激活的, 随机生成的网络结构最具多样性. 对于输入层的神经元, 其保留率通常
设为更接近 1 的数,
使得输入变化不会太大. 对输入层神经元进行丢弃时,
相当于
给数据增加噪声, 以此来提高网络的鲁棒性.
丢弃法一般是针对神经元进行随机丢弃, 但是也可以扩展到对神经元之间
的连接进行随机丢弃 [Wan et al., 2013] , 或每一层进行随机丢弃. 图 7.12 给出了
一个网络应用丢弃法后的示例