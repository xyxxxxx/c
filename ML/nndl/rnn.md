在前馈神经网络中，信息的传递是单向的，这种限制虽然使得网络变得更容易学习，但在一定程度上也减弱了神经网络模型的能力。在生物神经网络中神经元之间的连接关系要复杂得多。前馈神经网络可以看作一个复杂的函数，每次输入都是独立的，即网络的输出只依赖于当前的输入。但是在很多现实任务中，网络的输出不仅和当前时刻的输入相关，也和其过去一段时间的输出相关，比如一个有穷自动机，其下一个时刻的状态(输出)不仅仅和当前输入相关，也和当前状态(上一个时刻的输出)相关。此外，前馈网络难以处理时序数据，比如视频、 语音、 文本等，时序数据的长度一般是不固定的，而前馈神经网络要求输入和输出的维数都是固定的。因此，当处理这一类和时序数据相关的问题时就需要一种能力更强的模型。

循环神经网络(Recurrent Neural Network , RNN)是一类具有短期记忆能力的神经网络。在循环神经网络中, 神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构。和前馈神经网络相比，循环神经网络更加符合生物神经网络的结构。循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上。循环神经网络的参数学习可以通过随时间反向传播算法 [Werbos, 1990] 来学习，随时间反向传播算法即按照时间的逆序将错误信息一步步地往前传递，但当输入序列比较长时, 会存在梯度爆炸和消失问题 [Bengio et al., 1994; Hochreiter et al., 1997, 2001] ，也称为长程依赖问题。为了解决这个问题，人们对循环神经网络进行了很多的改进，其中最有效的改进方式引入门控机制(gating mechanism)。

此外，循环神经网络可以很容易地扩展到两种更广义的记忆网络模型：递归神经网络和图网络。





# 为网络增加记忆能力

为了处理这些时序数据并利用其历史信息，我们需要让网络具有短期记忆能力。以下三种方法可以用于为网络增加短期记忆能力。



## 延时神经网络

一种简单的利用历史信息的方法是建立一个额外的延时单元，用来存储网络的历史信息(可以包括输入、 输出、 隐状态等)。比较有代表性的模型是**延时神经网络(Time Delay Neural Network , TDNN)** [Lang et al., 1990; Waibel et al.,1989]。

延时神经网络在前馈网络中的每个非输出层的神经元上都添加一个延时器，记录神经元的最近几次活性值。在第$$t$$个时刻，第$$l$$层神经元的活性值依赖于第$$l − 1$$层神经元的最近$$K$$个时刻的活性值，即
$$
\pmb h_t^{(l)}=f(\pmb h_t^{(l-1)},\pmb h_{t-1}^{(l-1)},\cdots,\pmb h_{t-K}^{(l-1)})
$$
其中$$\pmb h_{t}^{(l)}\in \R^{M_l}$$表示第$$l$$层神经元在$$t$$时刻的活性值，$$M_l$$为第$$l$$层的神经元数量。



## 有外部输入的非线性自回归模型

**自回归(Auto Regressive, AR)**模型是统计学上常用的一类时间序列模型，预测变量$$\pmb y_t$$时使用它的历史信息
$$
\pmb y_t=w_0+\sum_{k=1}^Kw_k\pmb y_{t-k}+\varepsilon_t
$$
其中$$K$$为超参数，$$w_0 , ⋯ , w_K$$为可学习参数，$$ε_t ∼ N(0, σ^2 )$$为第 t 个时刻的噪声，方差$$σ^2$$和时间无关。

**有外部输入的非线性自回归模型(Nonlinear Auto Regressive with Exogenous Inputs Model , NARX)** [Leontaritis et al., 1985] 是自回归模型的扩展，在每个时刻$$\pmb t$$都有一个外部输入$$\pmb x_t$$，产生一个输出$$\pmb y_t$$。NARX 通过一个延时器记录最近$$K_x$$次的外部输入和最近$$K_y$$次的输出，第$$t$$个时刻的输出$$\pmb y_t$$为
$$
\pmb y_t=f(\pmb x_t,\pmb x_{t-1},\cdots,\pmb x_{t-K_x},\pmb y_{t-1},\pmb y_{t-2},\cdots,\pmb y_{t-K_y})
$$
其中$$f(⋅)$$表示非线性函数，可以是一个前馈网络，$$K_x$$和$$K_y$$为超参数。



## 循环神经网络

**循环神经网络(Recurrent Neural Network, RNN)**通过使用带自反馈的神经元，能够处理任意长度的时序数据。

给定一个输入序列$$\pmb x_{1:T}=(\pmb x_1,\pmb x_2,\cdots,\pmb x_t,\cdots,\pmb x_T)$$，循环神经网络通过以下公式更新带反馈边的隐藏层的活性值$$\pmb h_t$$：
$$
\pmb h_t=f(\pmb h_{t-1},\pmb x_t)
$$
其中$$\pmb h_0=0$$，$$f(\cdot)$$为一个非线性函数，可以是一个前馈网络。

下图给出了循环神经网络的示例，其中“延时器”为一个虚拟单元，用于记录神经元的最近一次（或几次）活性值。

![Screenshot from 2020-09-22 18-09-01.png](https://i.loli.net/2020/09/22/gQEepNbvfxJu6j7.png)

从数学上讲，公式$$\pmb h_t=f(\pmb h_{t-1},\pmb x_t)$$可以看作一个动力系统，因此隐藏层的活性值$$\pmb h_t$$在很多文献中也称为**状态(state)**或**隐状态(hidden state)**。

由于循环神经网络具有短期记忆能力，相当于存储装置， 因此其计算能力十分强大。理论上，循环神经网络可以近似任意的非线性动力系统。前馈神经网络可以模拟任何连续函数，而循环神经网络可以模拟任何程序。

> **动力系统(dynamical system)**是一个数学上的概念，指系统状态按照一定的规律随时间变化的系统。具体地讲，动力系统是使用一个函数来描述一个给定空间（如某个物理系统的状态空间）中所有点随时间的变化情况。生活中很多现象（比如钟摆晃动、 台球轨迹等）都可以动力系统来描述。





# 简单循环网络

**简单循环网络(Simple Recurrent Network ,SRN)** [Elman, 1990] 是一个非常简单的循环神经网络，只有一个隐藏层。

令向量$$\pmb x_t ∈ \R^M$$表示在时刻$$t$$时网络的输入，$$\pmb h_t ∈\R^D$$表示隐藏层状态（即隐藏层神经元活性值），则简单循环网络在时刻$$t$$的更新公式为
$$
\pmb z_t = \pmb U \pmb h_{t-1}+\pmb W\pmb x_t+\pmb b\\
\pmb h_t=f(\pmb z_t)
$$
其中$$\pmb z_t$$为隐藏层的净输入，$$\pmb U\in \R^{D\times D}$$为状态-状态权重矩阵，$$\pmb W\in \R^{D\times M}$$为状态-输入权重矩阵，$$\pmb b\in \R^D$$为偏置向量，$$f(\cdot)$$为非线性激活函数，通常为 Logistic 函数或 Tanh 函数。上式也经常合并为
$$
\pmb h_t = f(\pmb U \pmb h_{t-1}+\pmb W\pmb x_t+\pmb b)
$$
下图给出了按时间展开的循环神经网络

![Screenshot from 2020-09-22 18-30-07](/home/xyx/Pictures/Screenshot from 2020-09-22 18-30-07.png)



**完全连接**的循环神经网络定义为：网络输入$$\pmb x_t$$，输出$$\pmb y_t$$，动力系统为
$$
\pmb h_t = f(\pmb U \pmb h_{t-1}+\pmb W\pmb x_t+\pmb b)\\
\pmb y_t=\pmb V\pmb h_t
$$
其中$$\pmb h$$为状态，$$f(\cdot)$$为非线性激活函数，$$\pmb U,\pmb W,\pmb b,\pmb V$$为网络参数。



一个完全连接的循环网络是任何非线性动力系统的近似器：

**循环神经网络的通用近似定理** [Haykin, 2009] : 如果一个完全连接的循环神经网络有足够数量的 Sigmoid 型隐藏神经元，它可以以任意的准确率去近似任何一个非线性动力系统
$$
\pmb s_t=g(\pmb s_{t-1},\pmb x_t)\\
\pmb y_t=o(\pmb s_t)
$$
其中$$\pmb s_t$$为每个时刻的隐状态，$$\pmb x_t$$是外部输入，$$g(⋅)$$是可测的状态转换函数，$$o(⋅)$$是连续输出函数，并且对状态空间的紧致性没有限制。

> 证明略



一个完全连接的循环神经网络可以近似解决所有的可计算问题：

**循环神经网络的图灵完备定理**[Siegelmann et al., 1991] : 所有的图灵机都可以被一个由使用 Sigmoid 型激活函数的神经元构成的全连接循环网络来进行模拟。

> 证明略
>
> 图灵完备(Turing completeness)是指一种数据操作规则，可以实现图灵机(Turing machine)的所有功能，解决所有的可计算问题。目前主流的编程语言(比如 C++ 、Java 、Python 等)都是图灵完备的。





# 应用到机器学习

## 序列到类别模式

序列到类别模式中，输入为序列，输出为类别，比如在文本分类中，输入数据为单词的序列，输出为该文本的类别。

输入一个长度为$$T$$的序列$$\pmb x_{1∶T}=(\pmb x_1 , ⋯ ,\pmb x_T)$$，输出一个类别$$y ∈ \{1, ⋯ , C\}$$。我们将$$\pmb x_{1∶T}$$按时间顺序输入到循环神经网络中，得到对应时刻的状态序列$$(\pmb h_1 , ⋯ ,\pmb h_T)$$，然后将$$\pmb h_T$$作为输入序列的最终表示（或特
征）输入给分类器$$g(⋅)$$进行分类，即
$$
\hat{y}=g(\pmb h_T)
$$
其中$$g(⋅)$$可以是简单的线性分类器（比如 Logistic 回归）或复杂的分类器（比如多层前馈神经网络）。

另一种方法是对状态序列中的所有状态的平均作为输入序列的表示，即
$$
\hat{y}=g(\frac{1}{T}\sum_{t=1}^T\pmb h_t)
$$
![Screenshot from 2020-09-22 18-56-02.png](https://i.loli.net/2020/09/22/cSkCjL2N9Xzb8RV.png)



## 同步的序列到序列模式

同步的序列到序列模式主要用于序列标注(sequence labeling)任务，即每一时刻都有输入和输出，输入序列和输出序列的长度相同，比如在词性标注(part-of-speech tagging)中，每输入一个单词都输出其对应的词性标签。

输入一个长度为$$T$$的序列$$\pmb x_{1∶T}=(\pmb x_1 , ⋯ ,\pmb x_T)$$，输出一个序列$$\pmb y_{1∶T}=(\pmb y_1 , ⋯ ,\pmb y_T)$$。将每个状态$$\pmb h_t$$输入给分类器$$g(⋅)$$进行分类，即
$$
\hat{y}_t=g(\pmb h_t),\quad t=1,2,\cdots,T
$$
![Screenshot from 2020-09-22 19-05-12.png](https://i.loli.net/2020/09/22/9DRjqSx65gI4JL1.png)



## 异步的序列到序列模式

异步的序列到序列模式也称为编码器 - 解码器(encoder-decoder)模型，即输入序列和输出序列不需要有严格的对应关系，也不需要保持相同的长度，比如在机器翻译中，输入为源语言的单词序列，输出为目标语言的单词序列。